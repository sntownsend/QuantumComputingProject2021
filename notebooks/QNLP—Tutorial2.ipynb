{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4e500c",
   "metadata": {},
   "source": [
    "# Introduction to Quantum NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9faf3a",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "In the digital world, as we interact more and more with computers, it is increasingly necessary for computers to be able to understand our language. This is the job of the field of Natural Language Processing (NLP). Despite the fact that humans are so adept at acquiring and understanding human language, there are a variety of complicating factors making it more difficult to teach a computer to do the same.\n",
    "\n",
    "Currently, the most popular statistical approaches to NLP are based on __Bag of Words__. In Bag-of-Words models of language, words are represented as one-hot vectors of dimension $N$, where $N$ is the number of words in your vocabulary. A document is then constructed by adding up all the vectors for the words in the document. Given a corpus consisting of a large number of documents ($M$), dimensionality reduction techniques such as SVD are used to reduce the vector space dimension to $N^\\prime$ where $N^\\prime << N$. For example, Google's popular Word2Vec algorithm has a vocabulary size of $N =  929022$, which has been reduced down to a vector of dimension 300. Word2Vec algorithms are effective and relatively easy to create, but a major limitation of Bag of Words is that the representation does not take into account any of the context of the words within a sentence or the larger document.\n",
    "\n",
    "Newer statistical NLP models, such as BERT or GPT-3 model, model the context of words. However, this expanded representatio comes at a huge cost in terms of the number of parameters that need to be fitted. For example, GPT-3 requires fitting nearly 200 billion (!) parameters.\n",
    "\n",
    "Independent of these mainstream statistical NLP techniques, Joachim Lambek developed a mathematical grammar of language inspired by category theory and quantum mechanics. Lambek's grammar models lead directly to QNLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4da1a",
   "metadata": {},
   "source": [
    "## Why QNLP? Word Sense Disambiguation\n",
    "\n",
    "One of the reasons that statistical models such as GTP-3 require so many parameters, is that natural language is built up from words which are intrinsically ambiguous. Words usually have multiple different meanings. For example, the word \"set\" currently holds the Guinness World Record for English Word with the Most Meanings, with 430 senses listed in the 1989 Oxford English Dictionary. This is something humans can usually interpret with ease, but how do you teach a computer to do the same?\n",
    "\n",
    "To take a simpler example, the word \"file\" has fourteen senses. Determining which sense should be used can sometimes be narrowed down. For example, if \"file\" is marked for tense or aspect, e.g. \"filed\" or \"filing\", this indicates that it can be restricted to just the verbal meanings, of which there are three more general and three more specific. \n",
    "\n",
    "\n",
    "Take the sentences \"She filed charges against you\" and \"He filed his teeth every day\". We intuitively grasp the different meanings of the verbs in these sentences, despite the fact that they are pronounced and spelled the same, because of the extra information provided in the sentence, in this case the object of the verb. One job of natural language processing (NLP) is to teach a computer to do the same.\n",
    "\n",
    "\n",
    "The ambiguity of words is precisely why quantum computating is a natural approach for NLP. \n",
    "\n",
    ">[W]hat quantum theory and natural language share at a fundamental\n",
    "level is an interaction structure. This interaction structure, together with the specification\n",
    "of the spaces where the states live, determines the entire structure of processes of a theory.\n",
    "So the fact that quantum theory and natural language also share the use of vector spaces\n",
    "for describing states—albeit for very different reasons—makes those two theories (essentially)\n",
    "coincide. Therefore we say that QNLP is ‘quantum-native’ (Coecke et al. p. 20)\n",
    "\n",
    "In this tutorial I will illustrate how quantum computing can be used to disambiguate words. To do this, I will draw upon (with permission) the recent Master's Thesis of Thomas Hoffmann at Chalmers University."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30d0fd",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "As with most applications, a QNLP workflow will consist of a number of steps, some of which are done with classical computers and some of which would be done with quantum computers. Here I outline the basic workflow\n",
    "\n",
    "- Tag each word in a sentence with its Part of Speach (PoS)\n",
    "- Diagram each sentence according to the POS-tagging\n",
    "- Diagram simplification \n",
    "- Instantiate words as quantum features in circuit\n",
    "- Translate each simplified diagram to a quantum circuit with words instantiated as quantum features.\n",
    "- Compile and run each circuit on a quantum simulator.\n",
    "\n",
    "## Each of these steps will be illustrated in the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56376e",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "Part-of-speech (POS) tagging is done using classical computing. A popular NLP package that does this quite well is the Python package [spaCy](spacy.io). For this demonstration, go to [this notebook](pos_tagging.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc693bd8",
   "metadata": {},
   "source": [
    "## [Word Vectors](dimensionalty_reduction.ipynb)\n",
    "\n",
    "- [wikipedia2vec](https://wikipedia2vec.github.io/wikipedia2vec/)\n",
    "creates word vectors based on Wikipedia data dumps using a method similar to that described for Word2Vec above, but with a resulting dimensionality of 100.\n",
    "\n",
    "For example, the word vector for \"account\" is\n",
    "\n",
    "```Python\n",
    "account = \\\n",
    "[ 0.26, -0.06,  0.01,  0.35, -0.03,  0.04,  0.16,  0.14,  0.4 ,\n",
    "        0.02,  0.49, -0.19,  0.17,  0.2 ,  0.46,  0.12, -0.07,  0.07,\n",
    "        0.33, -0.31,  0.23, -0.35,  0.03,  0.51, -0.46, -0.16, -0.55,\n",
    "       -0.48,  0.4 ,  0.35, -0.02, -0.09,  0.34,  0.71, -0.16, -0.22,\n",
    "        0.23,  0.42,  0.09,  0.23, -0.1 , -0.23, -0.5 ,  0.32,  0.07,\n",
    "        0.25, -0.19,  0.33, -0.27,  0.29,  0.13, -0.06,  0.17,  0.58,\n",
    "        0.12,  0.05, -0.17,  0.17,  0.  ,  0.2 , -0.19, -0.25,  0.07,\n",
    "        0.26,  0.59, -0.65, -0.33, -0.31, -0.31, -0.39,  0.05,  0.36,\n",
    "        0.13,  0.31, -0.27,  0.51, -0.12,  0.29, -0.06, -0.3 , -0.41,\n",
    "       -0.48,  0.26, -0.01,  0.36,  0.63, -0.09, -0.15,  0.19,  0.09,\n",
    "        0.28,  0.15,  0.24, -0.04, -0.38, -0.63, -0.  ,  0.04, -0.25,\n",
    "        0.2 ]\n",
    "```\n",
    "\n",
    "The word vector for each 35 words in our vocabulary are computed and used to generate a $35\\times 100$ matrix. [Dimensionality reduction](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) is then used to reduce this matrix to a $35\\times 4$ matrix.\n",
    "\n",
    "In this reduced dimensional space, \"account\" is now represented by the vector:\n",
    "\n",
    "```Python\n",
    "[-0.92, -0.29, -0.023, 0.28]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14979e5c",
   "metadata": {},
   "source": [
    "## Generating Sentence Diagrams\n",
    "\n",
    "Click on [this notebook](SentenceDiagramming.ipynb) to see how to generate sentence diagrams.\n",
    "\n",
    "```Python\n",
    "sentences = [\n",
    "    \"file account\",\n",
    "    \"register account\",\n",
    "    \"smooth account\",\n",
    "    \"file nail\",\n",
    "    \"register nail\",\n",
    "    \"smooth nail\",\n",
    "    \"file charge_n\",\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7419d",
   "metadata": {},
   "source": [
    "## Diagram Simplification\n",
    "\n",
    "- Graphical Category Lanugages based on quantum physics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e737bb",
   "metadata": {},
   "source": [
    "## Key components\n",
    "\n",
    "- Word representation\n",
    "- Ambiguity Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f195b05",
   "metadata": {},
   "source": [
    "## Example 1-Qubit Model\n",
    "\n",
    "- Two state verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac3bba",
   "metadata": {},
   "source": [
    "\n",
    "Hoffmann's master's thesis overview - his code \n",
    "How does it work - let's step through and learn about it\n",
    "\n",
    "So with \"sentences\", there's the plausibility score as well as the cosine similarity measurement of the model's output to the expected output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cea1b4",
   "metadata": {},
   "source": [
    "#### 1 word with two meanings\n",
    "\n",
    "#### 4 words each with two meanings\n",
    "\n",
    "#### 1 word with three meanings (possibilities: pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f9258",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "First run a genetic algorithm for 25 iterations to get a good first estimate of the parameters (note: genetic algorithms are expensive). Then use a noisy optimzation algorithm to account for the noise of quantum computers to improve further over 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3753791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python (qnlp-ws)",
   "language": "python",
   "name": "qnlp-ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
